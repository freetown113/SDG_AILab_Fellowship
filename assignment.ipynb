{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import gensim\n",
    "import spacy\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Task 0 - Dataset\n",
    "I download a dataset with 17 SDG and filter it to work with SGD16 only. I also preliminary remove all unnecessary symbols from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (37575, 7)\n",
      "                          doi                           text_id  \\\n",
      "0  10.6027/9789289342698-7-en  00021941702cd84171ff33962197ca1f   \n",
      "1        10.18356/eca72908-en  00028349a7f9b2485ff344ae44ccfd6b   \n",
      "2  10.1787/9789264289062-4-en  0004eb64f96e1620cd852603d9cbe4d4   \n",
      "3     10.1787/5k9b7bn5qzvd-en  0006a887475ccfa5a7f5f51d4ac83d02   \n",
      "4  10.1787/9789264258211-6-en  0006d6e7593776abbdf4a6f985ea6d95   \n",
      "\n",
      "                                                text sdg labels_negative  \\\n",
      "0  \"From a gender perspective, Paulgaard points o...   5               1   \n",
      "1  Labour legislation regulates maximum working h...  11               2   \n",
      "2  The average figure also masks large difference...   3               1   \n",
      "3  The extent to which they are akin to corruptio...   3               1   \n",
      "4  A region reporting a higher rate will not earn...   3               2   \n",
      "\n",
      "  labels_positive           agreement  \n",
      "0               8  0.7777777777777778  \n",
      "1               1  0.3333333333333333  \n",
      "2               8  0.7777777777777778  \n",
      "3               2  0.3333333333333333  \n",
      "4               2                 0.0  \n",
      "                                             doi  \\\n",
      "10                     10.1017/S0008423907070424   \n",
      "16                   10.1016/J.PROCS.2017.11.303   \n",
      "25                      10.1176/APPI.PS.52.4.477   \n",
      "28     10.1093/ACPROF:OSO/9780199644315.001.0001   \n",
      "32                             10.25123/VEJ.1421   \n",
      "...                                          ...   \n",
      "37523                          10.1111/CFS.12125   \n",
      "37527              10.1080/21548455.2016.1157644   \n",
      "37549                  10.1017/S1368980007246622   \n",
      "37567                 10.1177/1037969X1103600105   \n",
      "37571                  10.1017/S0020589318000167   \n",
      "\n",
      "                                text_id  \\\n",
      "10     00162fc8346ca9cd525d8f87ac2b5352   \n",
      "16     00283156eb3e8de39758e3c5371a1340   \n",
      "25     0030671438d7acdbf2e63bab71ca4a2b   \n",
      "28     0032a7084ab889678001ab691549ad21   \n",
      "32     0035ee6937246822abd7a7dabe936e17   \n",
      "...                                 ...   \n",
      "37523  ffa9506ac24ab1d69d12cb3da5ebe750   \n",
      "37527  ffb6487ea13b3c31dd81e11539ac7740   \n",
      "37549  ffd83f7115c3571c0568b91f80280827   \n",
      "37567  fff761c81b12da6021373a04551f255a   \n",
      "37571  fffdf2bc9ac129ed95ef7727584aa74b   \n",
      "\n",
      "                                                    text sdg labels_negative  \\\n",
      "10     The “War on Terror” and the Framework of Inter...  16               0   \n",
      "16     Abstract   This paper demonstrates how differe...  16               1   \n",
      "25     Mental health courts are emerging in communiti...  16               2   \n",
      "28     1. Introduction: The Structures of the Crimina...  16               0   \n",
      "32     Abstract      The   negative effect of the dec...  16               1   \n",
      "...                                                  ...  ..             ...   \n",
      "37523  In a climate of austerity, timescales and targ...  16               0   \n",
      "37527  ABSTRACTThis study investigated deference to s...  16               2   \n",
      "37549  We offer a critique of Canada's approach to do...  16               1   \n",
      "37567  Unlike the right to food and housing, the righ...  16               1   \n",
      "37571  This article notes the judgment in Sophocleous...  16               2   \n",
      "\n",
      "      labels_positive           agreement  \n",
      "10                  3                 1.0  \n",
      "16                  2  0.3333333333333333  \n",
      "25                  2                 0.0  \n",
      "28                  3                 1.0  \n",
      "32                  3                 0.5  \n",
      "...               ...                 ...  \n",
      "37523               3                 1.0  \n",
      "37527               1  0.3333333333333333  \n",
      "37549               2  0.3333333333333333  \n",
      "37567               3                 0.5  \n",
      "37571               1  0.3333333333333333  \n",
      "\n",
      "[5105 rows x 7 columns]\n",
      "[' From a gender perspective Paulgaard points out that the labour markets of the fishing villages have been highly gender segregated in terms of the existence of male jobs and female jobs however the new business opportunities have led to the male population of the peripheral areas now working in the service industry in former female jobs That boys and girls are doing the same jobs indicates change because traditional boundaries between women and men s work are being crossed But the fact that young people are still working represents continuity with the past Paulgaard 2002 102 When Paulgaard refers to continuity with traditions she refers to the expectations of young adults to participate in adult culture thus these fishing villages traditionally have no actual youth culture As described earlier Paulgaard 2015 concludes that in some of Norway s peripheral areas school is still foreign a time waster stealing time from young adults who should instead spend their time on what is considered to be real work ']\n"
     ]
    }
   ],
   "source": [
    "df_osdg = pd.read_csv('C:/Users/Anjou/Downloads/osdg-community-data-v2022-10-01.csv', sep='\\t')\n",
    "df_osdg[df_osdg.columns[0].split('\\t')] = df_osdg.iloc[:,0].str.split('\\t', expand=True)\n",
    "df_osdg.drop(df_osdg.columns[0], axis=1, inplace=True)\n",
    "\n",
    "print('Shape:', df_osdg.shape)\n",
    "print(df_osdg.head())\n",
    "print(df_osdg[df_osdg['sdg']=='16'])\n",
    "\n",
    "data = df_osdg.text.values.tolist()\n",
    "data = [re.sub('[^A-Za-z0-9]+', ' ', sent) for sent in data]\n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Task 1 - Topic Modelling and Optimization\n",
    "\n",
    "For Topic Modelling I decided to use Latent Dirichlet Allocation (LDA). LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities. The purpose of LDA is mapping each document in our corpus to a set of topics which covers a good deal of the words in the document. As it doesn't require training, LDA is a convenient way to get a fast and relatively simple initial approach to the text analysis. LDA gives me a collection of documents that the algorithm has grouped together, as well as clusters of words and expressions that it used to infer these relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'gender', 'perspective', 'paulgaard', 'points', 'out', 'that', 'the', 'labour', 'markets', 'of', 'the', 'fishing', 'villages', 'have', 'been', 'highly', 'gender', 'segregated', 'in', 'terms', 'of', 'the', 'existence', 'of', 'male', 'jobs', 'and', 'female', 'jobs', 'however', 'the', 'new', 'business', 'opportunities', 'have', 'led', 'to', 'the', 'male', 'population', 'of', 'the', 'peripheral', 'areas', 'now', 'working', 'in', 'the', 'service', 'industry', 'in', 'former', 'female', 'jobs', 'that', 'boys', 'and', 'girls', 'are', 'doing', 'the', 'same', 'jobs', 'indicates', 'change', 'because', 'traditional', 'boundaries', 'between', 'women', 'and', 'men', 'work', 'are', 'being', 'crossed', 'but', 'the', 'fact', 'that', 'young', 'people', 'are', 'still', 'working', 'represents', 'continuity', 'with', 'the', 'past', 'paulgaard', 'when', 'paulgaard', 'refers', 'to', 'continuity', 'with', 'traditions', 'she', 'refers', 'to', 'the', 'expectations', 'of', 'young', 'adults', 'to', 'participate', 'in', 'adult', 'culture', 'thus', 'these', 'fishing', 'villages', 'traditionally', 'have', 'no', 'actual', 'youth', 'culture', 'as', 'described', 'earlier', 'paulgaard', 'concludes', 'that', 'in', 'some', 'of', 'norway', 'peripheral', 'areas', 'school', 'is', 'still', 'foreign', 'time', 'waster', 'stealing', 'time', 'from', 'young', 'adults', 'who', 'should', 'instead', 'spend', 'their', 'time', 'on', 'what', 'is', 'considered', 'to', 'be', 'real', 'work']]\n",
      "['gender perspective paulgaard point labour market fishing village gender segregate term existence job job business opportunity lead population area work service industry job boy girl do job indicate change boundary woman man work cross fact people work represent continuity paulgaard paulgaard refer continuity tradition refer expectation adult participate adult culture fishing village youth culture describe paulgaard conclude area school time waster steal time adult spend time consider work', 'labour legislation regulate work hour safety wage benefit worker prevention child labour enforcement immigration law country apply worker include service']\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "# Here I remove from data all words except nouns and verbs\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=[\"NOUN\", \"VERB\"]) #select noun and verb\n",
    "print(data_lemmatized[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(learning_method='online', n_components=20, n_jobs=-1,\n",
      "                          random_state=100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, n_jobs=-1, random_state=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, n_jobs=-1, random_state=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', n_jobs=-1, random_state=100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,\n",
    "                             stop_words='english',             \n",
    "                             lowercase=True,                   \n",
    "                             token_pattern='[a-zA-Z0-9]{3,}') \n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=20,max_iter=10,learning_method='online',random_state=100,batch_size=128,evaluate_every = -1,n_jobs = -1,               )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "print(lda_model)\n",
    "\n",
    "\n",
    "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    " evaluate_every=-1, learning_decay=0.7,\n",
    " learning_method=\"online\", learning_offset=10.0,\n",
    " max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
    " n_components=10, n_jobs=-1, perp_tol=0.1,\n",
    " random_state=100, topic_word_prior=None,\n",
    " total_samples=1000000.0, verbose=0)\n",
    "\n",
    "print(f'The LDA was launched with params: {lda_model.get_params()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -15291625.518245563\n",
      "Perplexity:  3206.432822988328\n"
     ]
    }
   ],
   "source": [
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized)) # Better result is the higher one\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized)) # Better result is the lower one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm starting to seek optimal hyperparameters for LDA. I could use some sophisticated algorithm from optuna, but here simple greed search works sufficiently well in this case. So I define two lists of parameters for \"n_components\" and \"learning_decay\" and launch GridSearch to find the optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.7, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -3062591.3089826843\n",
      "Model Perplexity:  2628.097774900861\n"
     ]
    }
   ],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [5, 10, 20], 'learning_decay': [0.5, 0.7, 0.9]}\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation(max_iter=5, learning_method='online', learning_offset=50.,random_state=31)\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)\n",
    "GridSearchCV(cv=None, error_score='raise',\n",
    "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
    "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
    "             mean_change_tol=0.001, n_components=10, n_jobs=1, perp_tol=0.1, random_state=None,\n",
    "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
    "        n_jobs=1,\n",
    "       param_grid={'n_components': [5, 10], 'learning_decay': [0.8, 0.9]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring=None, verbose=0)\n",
    "\n",
    "\n",
    "best_lda_model = model.best_estimator_\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
    "lda_model = best_lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6ed07_row0_col0, #T_6ed07_row0_col3, #T_6ed07_row0_col5, #T_6ed07_row1_col1, #T_6ed07_row1_col3, #T_6ed07_row1_col4, #T_6ed07_row1_col5, #T_6ed07_row2_col2, #T_6ed07_row2_col5, #T_6ed07_row3_col1, #T_6ed07_row3_col2, #T_6ed07_row3_col4, #T_6ed07_row3_col5, #T_6ed07_row4_col2, #T_6ed07_row4_col3, #T_6ed07_row4_col5, #T_6ed07_row5_col2, #T_6ed07_row5_col3, #T_6ed07_row5_col4, #T_6ed07_row5_col5, #T_6ed07_row6_col0, #T_6ed07_row6_col1, #T_6ed07_row6_col4, #T_6ed07_row6_col5, #T_6ed07_row7_col0, #T_6ed07_row7_col1, #T_6ed07_row7_col5, #T_6ed07_row8_col0, #T_6ed07_row8_col1, #T_6ed07_row8_col2, #T_6ed07_row9_col0, #T_6ed07_row9_col1, #T_6ed07_row9_col2, #T_6ed07_row10_col4, #T_6ed07_row10_col5, #T_6ed07_row11_col0, #T_6ed07_row11_col1, #T_6ed07_row11_col2, #T_6ed07_row12_col2, #T_6ed07_row12_col3, #T_6ed07_row12_col5, #T_6ed07_row13_col2, #T_6ed07_row13_col3, #T_6ed07_row13_col5, #T_6ed07_row14_col0, #T_6ed07_row14_col1, #T_6ed07_row14_col2, #T_6ed07_row14_col3, #T_6ed07_row14_col5, #T_6ed07_row15_col0, #T_6ed07_row15_col1, #T_6ed07_row15_col5, #T_6ed07_row16_col0, #T_6ed07_row16_col3, #T_6ed07_row16_col4, #T_6ed07_row16_col5, #T_6ed07_row17_col0, #T_6ed07_row17_col1, #T_6ed07_row17_col5, #T_6ed07_row18_col1, #T_6ed07_row18_col2, #T_6ed07_row18_col5, #T_6ed07_row19_col0, #T_6ed07_row19_col1, #T_6ed07_row19_col4 {\n",
       "  color: yellow;\n",
       "  font-weight: 700;\n",
       "}\n",
       "#T_6ed07_row0_col1, #T_6ed07_row0_col2, #T_6ed07_row0_col4, #T_6ed07_row1_col0, #T_6ed07_row1_col2, #T_6ed07_row2_col0, #T_6ed07_row2_col1, #T_6ed07_row2_col3, #T_6ed07_row2_col4, #T_6ed07_row3_col0, #T_6ed07_row3_col3, #T_6ed07_row4_col0, #T_6ed07_row4_col1, #T_6ed07_row4_col4, #T_6ed07_row5_col0, #T_6ed07_row5_col1, #T_6ed07_row6_col2, #T_6ed07_row6_col3, #T_6ed07_row7_col2, #T_6ed07_row7_col3, #T_6ed07_row7_col4, #T_6ed07_row8_col3, #T_6ed07_row8_col4, #T_6ed07_row8_col5, #T_6ed07_row9_col3, #T_6ed07_row9_col4, #T_6ed07_row9_col5, #T_6ed07_row10_col0, #T_6ed07_row10_col1, #T_6ed07_row10_col2, #T_6ed07_row10_col3, #T_6ed07_row11_col3, #T_6ed07_row11_col4, #T_6ed07_row11_col5, #T_6ed07_row12_col0, #T_6ed07_row12_col1, #T_6ed07_row12_col4, #T_6ed07_row13_col0, #T_6ed07_row13_col1, #T_6ed07_row13_col4, #T_6ed07_row14_col4, #T_6ed07_row15_col2, #T_6ed07_row15_col3, #T_6ed07_row15_col4, #T_6ed07_row16_col1, #T_6ed07_row16_col2, #T_6ed07_row17_col2, #T_6ed07_row17_col3, #T_6ed07_row17_col4, #T_6ed07_row18_col0, #T_6ed07_row18_col3, #T_6ed07_row18_col4, #T_6ed07_row19_col2, #T_6ed07_row19_col3, #T_6ed07_row19_col5 {\n",
       "  color: blue;\n",
       "  font-weight: 400;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6ed07\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6ed07_level0_col0\" class=\"col_heading level0 col0\" >Topic0</th>\n",
       "      <th id=\"T_6ed07_level0_col1\" class=\"col_heading level0 col1\" >Topic1</th>\n",
       "      <th id=\"T_6ed07_level0_col2\" class=\"col_heading level0 col2\" >Topic2</th>\n",
       "      <th id=\"T_6ed07_level0_col3\" class=\"col_heading level0 col3\" >Topic3</th>\n",
       "      <th id=\"T_6ed07_level0_col4\" class=\"col_heading level0 col4\" >Topic4</th>\n",
       "      <th id=\"T_6ed07_level0_col5\" class=\"col_heading level0 col5\" >dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row0\" class=\"row_heading level0 row0\" >Doc0</th>\n",
       "      <td id=\"T_6ed07_row0_col0\" class=\"data row0 col0\" >0.150000</td>\n",
       "      <td id=\"T_6ed07_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row0_col2\" class=\"data row0 col2\" >0.050000</td>\n",
       "      <td id=\"T_6ed07_row0_col3\" class=\"data row0 col3\" >0.800000</td>\n",
       "      <td id=\"T_6ed07_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row0_col5\" class=\"data row0 col5\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row1\" class=\"row_heading level0 row1\" >Doc1</th>\n",
       "      <td id=\"T_6ed07_row1_col0\" class=\"data row1 col0\" >0.090000</td>\n",
       "      <td id=\"T_6ed07_row1_col1\" class=\"data row1 col1\" >0.270000</td>\n",
       "      <td id=\"T_6ed07_row1_col2\" class=\"data row1 col2\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row1_col3\" class=\"data row1 col3\" >0.450000</td>\n",
       "      <td id=\"T_6ed07_row1_col4\" class=\"data row1 col4\" >0.180000</td>\n",
       "      <td id=\"T_6ed07_row1_col5\" class=\"data row1 col5\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row2\" class=\"row_heading level0 row2\" >Doc2</th>\n",
       "      <td id=\"T_6ed07_row2_col0\" class=\"data row2 col0\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row2_col1\" class=\"data row2 col1\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row2_col2\" class=\"data row2 col2\" >0.930000</td>\n",
       "      <td id=\"T_6ed07_row2_col3\" class=\"data row2 col3\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row2_col4\" class=\"data row2 col4\" >0.050000</td>\n",
       "      <td id=\"T_6ed07_row2_col5\" class=\"data row2 col5\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row3\" class=\"row_heading level0 row3\" >Doc3</th>\n",
       "      <td id=\"T_6ed07_row3_col0\" class=\"data row3 col0\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row3_col1\" class=\"data row3 col1\" >0.260000</td>\n",
       "      <td id=\"T_6ed07_row3_col2\" class=\"data row3 col2\" >0.220000</td>\n",
       "      <td id=\"T_6ed07_row3_col3\" class=\"data row3 col3\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row3_col4\" class=\"data row3 col4\" >0.510000</td>\n",
       "      <td id=\"T_6ed07_row3_col5\" class=\"data row3 col5\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row4\" class=\"row_heading level0 row4\" >Doc4</th>\n",
       "      <td id=\"T_6ed07_row4_col0\" class=\"data row4 col0\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row4_col1\" class=\"data row4 col1\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row4_col2\" class=\"data row4 col2\" >0.600000</td>\n",
       "      <td id=\"T_6ed07_row4_col3\" class=\"data row4 col3\" >0.310000</td>\n",
       "      <td id=\"T_6ed07_row4_col4\" class=\"data row4 col4\" >0.080000</td>\n",
       "      <td id=\"T_6ed07_row4_col5\" class=\"data row4 col5\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row5\" class=\"row_heading level0 row5\" >Doc5</th>\n",
       "      <td id=\"T_6ed07_row5_col0\" class=\"data row5 col0\" >0.060000</td>\n",
       "      <td id=\"T_6ed07_row5_col1\" class=\"data row5 col1\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row5_col2\" class=\"data row5 col2\" >0.510000</td>\n",
       "      <td id=\"T_6ed07_row5_col3\" class=\"data row5 col3\" >0.300000</td>\n",
       "      <td id=\"T_6ed07_row5_col4\" class=\"data row5 col4\" >0.130000</td>\n",
       "      <td id=\"T_6ed07_row5_col5\" class=\"data row5 col5\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row6\" class=\"row_heading level0 row6\" >Doc6</th>\n",
       "      <td id=\"T_6ed07_row6_col0\" class=\"data row6 col0\" >0.290000</td>\n",
       "      <td id=\"T_6ed07_row6_col1\" class=\"data row6 col1\" >0.330000</td>\n",
       "      <td id=\"T_6ed07_row6_col2\" class=\"data row6 col2\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row6_col4\" class=\"data row6 col4\" >0.370000</td>\n",
       "      <td id=\"T_6ed07_row6_col5\" class=\"data row6 col5\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row7\" class=\"row_heading level0 row7\" >Doc7</th>\n",
       "      <td id=\"T_6ed07_row7_col0\" class=\"data row7 col0\" >0.270000</td>\n",
       "      <td id=\"T_6ed07_row7_col1\" class=\"data row7 col1\" >0.710000</td>\n",
       "      <td id=\"T_6ed07_row7_col2\" class=\"data row7 col2\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row7_col3\" class=\"data row7 col3\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row7_col4\" class=\"data row7 col4\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row7_col5\" class=\"data row7 col5\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row8\" class=\"row_heading level0 row8\" >Doc8</th>\n",
       "      <td id=\"T_6ed07_row8_col0\" class=\"data row8 col0\" >0.470000</td>\n",
       "      <td id=\"T_6ed07_row8_col1\" class=\"data row8 col1\" >0.310000</td>\n",
       "      <td id=\"T_6ed07_row8_col2\" class=\"data row8 col2\" >0.200000</td>\n",
       "      <td id=\"T_6ed07_row8_col3\" class=\"data row8 col3\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row8_col4\" class=\"data row8 col4\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row8_col5\" class=\"data row8 col5\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row9\" class=\"row_heading level0 row9\" >Doc9</th>\n",
       "      <td id=\"T_6ed07_row9_col0\" class=\"data row9 col0\" >0.640000</td>\n",
       "      <td id=\"T_6ed07_row9_col1\" class=\"data row9 col1\" >0.240000</td>\n",
       "      <td id=\"T_6ed07_row9_col2\" class=\"data row9 col2\" >0.120000</td>\n",
       "      <td id=\"T_6ed07_row9_col3\" class=\"data row9 col3\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row9_col4\" class=\"data row9 col4\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row9_col5\" class=\"data row9 col5\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row10\" class=\"row_heading level0 row10\" >Doc10</th>\n",
       "      <td id=\"T_6ed07_row10_col0\" class=\"data row10 col0\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row10_col1\" class=\"data row10 col1\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row10_col2\" class=\"data row10 col2\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row10_col3\" class=\"data row10 col3\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row10_col4\" class=\"data row10 col4\" >0.990000</td>\n",
       "      <td id=\"T_6ed07_row10_col5\" class=\"data row10 col5\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row11\" class=\"row_heading level0 row11\" >Doc11</th>\n",
       "      <td id=\"T_6ed07_row11_col0\" class=\"data row11 col0\" >0.450000</td>\n",
       "      <td id=\"T_6ed07_row11_col1\" class=\"data row11 col1\" >0.330000</td>\n",
       "      <td id=\"T_6ed07_row11_col2\" class=\"data row11 col2\" >0.210000</td>\n",
       "      <td id=\"T_6ed07_row11_col3\" class=\"data row11 col3\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row11_col4\" class=\"data row11 col4\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row11_col5\" class=\"data row11 col5\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row12\" class=\"row_heading level0 row12\" >Doc12</th>\n",
       "      <td id=\"T_6ed07_row12_col0\" class=\"data row12 col0\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row12_col1\" class=\"data row12 col1\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row12_col2\" class=\"data row12 col2\" >0.670000</td>\n",
       "      <td id=\"T_6ed07_row12_col3\" class=\"data row12 col3\" >0.310000</td>\n",
       "      <td id=\"T_6ed07_row12_col4\" class=\"data row12 col4\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row12_col5\" class=\"data row12 col5\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row13\" class=\"row_heading level0 row13\" >Doc13</th>\n",
       "      <td id=\"T_6ed07_row13_col0\" class=\"data row13 col0\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row13_col1\" class=\"data row13 col1\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row13_col2\" class=\"data row13 col2\" >0.580000</td>\n",
       "      <td id=\"T_6ed07_row13_col3\" class=\"data row13 col3\" >0.410000</td>\n",
       "      <td id=\"T_6ed07_row13_col4\" class=\"data row13 col4\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row13_col5\" class=\"data row13 col5\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row14\" class=\"row_heading level0 row14\" >Doc14</th>\n",
       "      <td id=\"T_6ed07_row14_col0\" class=\"data row14 col0\" >0.230000</td>\n",
       "      <td id=\"T_6ed07_row14_col1\" class=\"data row14 col1\" >0.240000</td>\n",
       "      <td id=\"T_6ed07_row14_col2\" class=\"data row14 col2\" >0.220000</td>\n",
       "      <td id=\"T_6ed07_row14_col3\" class=\"data row14 col3\" >0.300000</td>\n",
       "      <td id=\"T_6ed07_row14_col4\" class=\"data row14 col4\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row14_col5\" class=\"data row14 col5\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row15\" class=\"row_heading level0 row15\" >Doc15</th>\n",
       "      <td id=\"T_6ed07_row15_col0\" class=\"data row15 col0\" >0.130000</td>\n",
       "      <td id=\"T_6ed07_row15_col1\" class=\"data row15 col1\" >0.860000</td>\n",
       "      <td id=\"T_6ed07_row15_col2\" class=\"data row15 col2\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row15_col3\" class=\"data row15 col3\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row15_col4\" class=\"data row15 col4\" >0.010000</td>\n",
       "      <td id=\"T_6ed07_row15_col5\" class=\"data row15 col5\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row16\" class=\"row_heading level0 row16\" >Doc16</th>\n",
       "      <td id=\"T_6ed07_row16_col0\" class=\"data row16 col0\" >0.140000</td>\n",
       "      <td id=\"T_6ed07_row16_col1\" class=\"data row16 col1\" >0.100000</td>\n",
       "      <td id=\"T_6ed07_row16_col2\" class=\"data row16 col2\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row16_col3\" class=\"data row16 col3\" >0.180000</td>\n",
       "      <td id=\"T_6ed07_row16_col4\" class=\"data row16 col4\" >0.570000</td>\n",
       "      <td id=\"T_6ed07_row16_col5\" class=\"data row16 col5\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row17\" class=\"row_heading level0 row17\" >Doc17</th>\n",
       "      <td id=\"T_6ed07_row17_col0\" class=\"data row17 col0\" >0.470000</td>\n",
       "      <td id=\"T_6ed07_row17_col1\" class=\"data row17 col1\" >0.510000</td>\n",
       "      <td id=\"T_6ed07_row17_col2\" class=\"data row17 col2\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row17_col3\" class=\"data row17 col3\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row17_col4\" class=\"data row17 col4\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row17_col5\" class=\"data row17 col5\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row18\" class=\"row_heading level0 row18\" >Doc18</th>\n",
       "      <td id=\"T_6ed07_row18_col0\" class=\"data row18 col0\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row18_col1\" class=\"data row18 col1\" >0.250000</td>\n",
       "      <td id=\"T_6ed07_row18_col2\" class=\"data row18 col2\" >0.700000</td>\n",
       "      <td id=\"T_6ed07_row18_col3\" class=\"data row18 col3\" >0.050000</td>\n",
       "      <td id=\"T_6ed07_row18_col4\" class=\"data row18 col4\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row18_col5\" class=\"data row18 col5\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ed07_level0_row19\" class=\"row_heading level0 row19\" >Doc19</th>\n",
       "      <td id=\"T_6ed07_row19_col0\" class=\"data row19 col0\" >0.650000</td>\n",
       "      <td id=\"T_6ed07_row19_col1\" class=\"data row19 col1\" >0.130000</td>\n",
       "      <td id=\"T_6ed07_row19_col2\" class=\"data row19 col2\" >0.060000</td>\n",
       "      <td id=\"T_6ed07_row19_col3\" class=\"data row19 col3\" >0.000000</td>\n",
       "      <td id=\"T_6ed07_row19_col4\" class=\"data row19 col4\" >0.160000</td>\n",
       "      <td id=\"T_6ed07_row19_col5\" class=\"data row19 col5\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25333e86dd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building Topic Matrix based on a resulted documents\n",
    "lda_output = lda_model.transform(data_vectorized)\n",
    "\n",
    "topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_components)]\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic[\"dominant_topic\"] = dominant_topic\n",
    "\n",
    "def color(val):\n",
    " color = \"yellow\" if val > .1 else \"blue\"\n",
    " return \"color: {col}\".format(col=color)\n",
    "def to_bold(val):\n",
    " weight = 700 if val > .1 else 400\n",
    " return \"font-weight: {weight}\".format(weight=weight)\n",
    "\n",
    "df_document_topics = df_document_topic.head(20).style.applymap(color).applymap(to_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>water</td>\n",
       "      <td>energy</td>\n",
       "      <td>use</td>\n",
       "      <td>land</td>\n",
       "      <td>production</td>\n",
       "      <td>costs</td>\n",
       "      <td>sector</td>\n",
       "      <td>investment</td>\n",
       "      <td>agricultural</td>\n",
       "      <td>market</td>\n",
       "      <td>Topic_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>development</td>\n",
       "      <td>policy</td>\n",
       "      <td>national</td>\n",
       "      <td>government</td>\n",
       "      <td>countries</td>\n",
       "      <td>public</td>\n",
       "      <td>climate</td>\n",
       "      <td>information</td>\n",
       "      <td>policies</td>\n",
       "      <td>management</td>\n",
       "      <td>Topic_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>countries</td>\n",
       "      <td>health</td>\n",
       "      <td>income</td>\n",
       "      <td>poverty</td>\n",
       "      <td>care</td>\n",
       "      <td>cent</td>\n",
       "      <td>population</td>\n",
       "      <td>average</td>\n",
       "      <td>oecd</td>\n",
       "      <td>rates</td>\n",
       "      <td>Topic_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>women</td>\n",
       "      <td>education</td>\n",
       "      <td>school</td>\n",
       "      <td>work</td>\n",
       "      <td>students</td>\n",
       "      <td>labour</td>\n",
       "      <td>gender</td>\n",
       "      <td>employment</td>\n",
       "      <td>schools</td>\n",
       "      <td>children</td>\n",
       "      <td>Topic_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>law</td>\n",
       "      <td>rights</td>\n",
       "      <td>international</td>\n",
       "      <td>social</td>\n",
       "      <td>human</td>\n",
       "      <td>political</td>\n",
       "      <td>article</td>\n",
       "      <td>public</td>\n",
       "      <td>legal</td>\n",
       "      <td>state</td>\n",
       "      <td>Topic_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word 0     Word 1         Word 2      Word 3      Word 4  \\\n",
       "Topic 0        water     energy            use        land  production   \n",
       "Topic 1  development     policy       national  government   countries   \n",
       "Topic 2    countries     health         income     poverty        care   \n",
       "Topic 3        women  education         school        work    students   \n",
       "Topic 4          law     rights  international      social       human   \n",
       "\n",
       "            Word 5      Word 6       Word 7        Word 8      Word 9   Topics  \n",
       "Topic 0      costs      sector   investment  agricultural      market  Topic_0  \n",
       "Topic 1     public     climate  information      policies  management  Topic_1  \n",
       "Topic 2       cent  population      average          oecd       rates  Topic_2  \n",
       "Topic 3     labour      gender   employment       schools    children  Topic_3  \n",
       "Topic 4  political     article       public         legal       state  Topic_4  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words per each topic visualisation\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=10):\n",
    "    keywords = np.array(vectorizer.get_feature_names_out())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=lda_model)\n",
    "\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords\n",
    "\n",
    "Topics = [\"Topic_\"+str(i) for i in range(lda_model.n_components)]\n",
    "df_topic_keywords[\"Topics\"]=Topics\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search showed that optimal parameter for n_components is 5, so I consider that 5 topics is the best choise for this data. About the number of the words I consider that 10 worlds is optimal to describe a topic. Sometimes even 5 words are enough but not for all of them. I can formulate topics according to the words:\n",
    "Topic_0. Energy and natural resources market\n",
    "Topic_1. Politics\n",
    "Topic_2. Standard of living\n",
    "Topic_3. Jobs and Education\n",
    "Topic_4. Legal and law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Task 2 - Topic Summarization via Text Summarization/Generation with a Large Language Model\n",
    "\n",
    "In this task I'm going to generate text summaries using the topics I generated in the previous task. In Task_1 I generated 5 topics each of them described by a number of words. Based on these words I will generate a text using a ready to use large model. I take a model \"distilgpt2\" from Hugginface. Distilgpt2 is a lighter version of GPT2 but in this case it should be enough. I downloaded the transformers library and used the module pipeline to generate text from topics.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\modeling_utils.py\", line 399, in load_state_dict\n",
      "    return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py\", line 713, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py\", line 930, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\serialization.py\", line 871, in persistent_load\n",
      "    obj = cast(Storage, torch._UntypedStorage(nbytes))\n",
      "RuntimeError: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 154389504 bytes.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Temp\\ipykernel_10788\\701016407.py\", line 5, in <module>\n",
      "    summarizer = pipeline(\"summarization\", model=\"distilgpt2\")\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\pipelines\\__init__.py\", line 711, in pipeline\n",
      "    framework, model = infer_framework_load_model(\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\pipelines\\base.py\", line 257, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\modeling_utils.py\", line 2182, in from_pretrained\n",
      "    state_dict = load_state_dict(resolved_archive_file)\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\modeling_utils.py\", line 403, in load_state_dict\n",
      "    if f.read().startswith(\"version\"):\n",
      "MemoryError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 1997, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 812, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 730, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 155, in _format_traceback_lines\n",
      "    line = stack_line.render(pygmented=has_colors).rstrip('\\n') + '\\n'\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\core.py\", line 391, in render\n",
      "    start_line, lines = self.frame_info._pygmented_scope_lines\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\core.py\", line 824, in _pygmented_scope_lines\n",
      "    lines = _pygmented_with_ranges(formatter, code, ranges)\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\utils.py\", line 165, in _pygmented_with_ranges\n",
      "    return pygments.highlight(code, lexer, formatter).splitlines()\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pygments\\__init__.py\", line 82, in highlight\n",
      "    return format(lex(code, lexer), formatter, outfile)\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pygments\\__init__.py\", line 61, in format\n",
      "    formatter.format(tokens, realoutfile)\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pygments\\formatters\\terminal256.py\", line 250, in format\n",
      "    return Formatter.format(self, tokensource, outfile)\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pygments\\formatter.py\", line 94, in format\n",
      "    return self.format_unencoded(tokensource, outfile)\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pygments\\formatters\\terminal256.py\", line 256, in format_unencoded\n",
      "    for ttype, value in tokensource:\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\utils.py\", line 158, in get_tokens\n",
      "    for ttype, value in super().get_tokens(text):\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pygments\\lexer.py\", line 190, in streamer\n",
      "    for _, t, v in self.get_tokens_unprocessed(text):\n",
      "  File \"C:\\Users\\Anjou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pygments\\lexer.py\", line 632, in get_tokens_unprocessed\n",
      "    m = rexmatch(text, pos)\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "# build summarization model \n",
    "summarizer = pipeline(\"summarization\")\n",
    "# create word cloud module\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=5, contour_color='steelblue')\n",
    "for i in range(lda_model.n_components):\n",
    "  long_string = ' '.join(df_topic_keywords.iloc[i, :].tolist()[:-1])\n",
    "  print(summarizer(long_string))\n",
    "  wordcloud.generate(long_string)\n",
    "  display(wordcloud.to_image())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the generated topic summaries is quite high. Here it's clear that even a simplified model such as distilgpt2 gives good results. I see from generated summaries that they provide appropriate information based on the topics provided.   "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b469cb7e38c7c0aa55d596c9b24d2eac317e06783a4c48e9b6c22843f2315771"
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
